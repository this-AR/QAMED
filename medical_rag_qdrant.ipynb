{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f88dc8",
   "metadata": {},
   "source": [
    "# Medical RAG System with Qdrant Vector Database\n",
    "\n",
    "This notebook implements a comprehensive Medical Retrieval-Augmented Generation (RAG) system that:\n",
    "- Processes medical textbooks (PDF) and creates searchable chunks\n",
    "- Uses Qdrant vector database for efficient document retrieval\n",
    "- Employs medical-specific embeddings (S-PubMedBert)\n",
    "- Implements query decomposition for complex medical questions\n",
    "- Generates accurate answers using Zephyr-7B model\n",
    "\n",
    "## System Architecture\n",
    "1. **Document Processing**: PDF → Chunks with metadata\n",
    "2. **Vector Storage**: Qdrant with medical embeddings\n",
    "3. **Query Processing**: FLAN-T5 for question decomposition\n",
    "4. **Retrieval**: Semantic search + BGE reranking\n",
    "5. **Generation**: Zephyr-7B for medical answer synthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c3736e",
   "metadata": {},
   "source": [
    "## 📦 Installation and Dependencies\n",
    "\n",
    "Install all required packages for the medical RAG system. This includes:\n",
    "- LangChain for document processing\n",
    "- Qdrant for vector database\n",
    "- Transformers for language models\n",
    "- Sentence-transformers for embeddings\n",
    "- PyMuPDF for PDF processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c9a38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain qdrant-client sentence-transformers transformers pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c614cd67",
   "metadata": {},
   "source": [
    "## Additional Community Package\n",
    "\n",
    "Install the LangChain community package for additional integrations and document loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a64d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548cf713",
   "metadata": {},
   "source": [
    "## 🔧 Core Imports and Setup\n",
    "\n",
    "Import all necessary libraries for the medical RAG system:\n",
    "- PyTorch for deep learning operations\n",
    "- Transformers for language models\n",
    "- LangChain for document processing\n",
    "- Qdrant for vector database operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d93e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    pipeline, AutoModelForSeq2SeqLM\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from langchain_community.document_loaders import PyMuPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513bc3ef",
   "metadata": {},
   "source": [
    "## 📚 Chapter Mapping Configuration\n",
    "\n",
    "Define the Table of Contents (TOC) mapping for the medical textbook.\n",
    "This allows us to automatically tag each document chunk with its corresponding chapter,\n",
    "enabling better organization and filtering during retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb3d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOC-based chapter mapping\n",
    "CHAPTER_MAP = {\n",
    "    (1, 9): \"Introduction and Overview of the Abdomen\",\n",
    "    (10, 23): \"Osteology of the Abdomen\",\n",
    "    (24, 45): \"Anterior Abdominal Wall\",\n",
    "    (46, 58): \"Inguinal Region/Groin\",\n",
    "    (59, 73): \"Male External Genital Organs\",\n",
    "    (74, 92): \"Abdominal Cavity and Peritoneum\",\n",
    "    (93, 108): \"Abdominal Part of Esophagus, Stomach, and Spleen\",\n",
    "    (109, 125): \"Liver and Extrahepatic Biliary Apparatus\",\n",
    "    (126, 143): \"Duodenum, Pancreas, and Portal Vein\",\n",
    "    (144, 164): \"Small and Large Intestines\",\n",
    "    (165, 184): \"Kidneys, Ureters, and Suprarenal Glands\",\n",
    "    (185, 201): \"Posterior Abdominal Wall and Associated Structures\",\n",
    "    (202, 211): \"Pelvis\",\n",
    "    (212, 224): \"Pelvic Walls and Associated Soft Tissue Structures\",\n",
    "    (225, 237): \"Perineum\",\n",
    "    (238, 250): \"Urinary Bladder and Urethra\",\n",
    "    (251, 259): \"Male Genital Organs\",\n",
    "    (260, 278): \"Female Genital Organs\",\n",
    "    (279, 290): \"Rectum and Anal Canal\",\n",
    "    (291, 298): \"Introduction to the Lower Limb\",\n",
    "    (299, 327): \"Bones of the Lower Limb\",\n",
    "    (328, 343): \"Front of the Thigh\",\n",
    "    (344, 352): \"Medial Side of the Thigh\",\n",
    "    (353, 363): \"Gluteal Region\",\n",
    "    (364, 376): \"Back of the Thigh and Popliteal Fossa\",\n",
    "    (377, 385): \"Hip Joint\",\n",
    "    (386, 399): \"Front of the Leg and Dorsum of the Foot\",\n",
    "    (400, 406): \"Lateral and Medial Sides of the Leg\",\n",
    "    (407, 419): \"Back of the Leg\",\n",
    "    (420, 431): \"Sole of the Foot\",\n",
    "    (432, 438): \"Arches of the Foot\",\n",
    "    (439, 457): \"Joints of the Lower Limb\",\n",
    "    (458, 466): \"Venous and Lymphatic Drainage of the Lower Limb\",\n",
    "    (467, 478): \"Innervation of the Lower Limb\"\n",
    "}\n",
    "\n",
    "def get_chapter_by_page(page_number):\n",
    "    for (start, end), title in CHAPTER_MAP.items():\n",
    "        if start <= page_number <= end:\n",
    "            return title\n",
    "    return \"Unknown Chapter\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791a043b",
   "metadata": {},
   "source": [
    "## 🏷️ Content Label Detection\n",
    "\n",
    "Implement automatic content labeling to categorize medical text chunks.\n",
    "This function analyzes text content and assigns labels such as:\n",
    "- @definition: Text containing definitions\n",
    "- @symptoms: Text describing symptoms\n",
    "- @diagnosis: Diagnostic information\n",
    "- @treatment: Treatment procedures\n",
    "- @anatomy_structure: Anatomical descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17301395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_labels(text):\n",
    "    labels = []\n",
    "    t = text.lower()\n",
    "    if \"is defined as\" in t or \"refers to\" in t or \"means\" in t:\n",
    "        labels.append(\"@definition\")\n",
    "    if \"symptoms include\" in t or \"signs are\" in t or \"manifestations\" in t:\n",
    "        labels.append(\"@symptoms\")\n",
    "    if \"diagnosis is based on\" in t or \"diagnosed by\" in t or \"investigations include\" in t:\n",
    "        labels.append(\"@diagnosis\")\n",
    "    if \"treatment includes\" in t or \"managed by\" in t or \"therapy\" in t:\n",
    "        labels.append(\"@treatment\")\n",
    "    if \"relations include\" in t or \"borders are\" in t:\n",
    "        labels.append(\"@anatomy_structure\")\n",
    "    if \"supplied by\" in t or \"innervated by\" in t:\n",
    "        labels.append(\"@supply\")\n",
    "    return labels or [\"@general\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f522c1c9",
   "metadata": {},
   "source": [
    "## 📄 PDF Processing and Chunking\n",
    "\n",
    "Process medical PDFs and create intelligent chunks with rich metadata.\n",
    "Each chunk includes:\n",
    "- Chapter information based on page number\n",
    "- Content labels for categorization\n",
    "- Unique chunk ID for tracking\n",
    "- Book metadata for source attribution\n",
    "\n",
    "Uses RecursiveCharacterTextSplitter with 250 character chunks and 50 character overlap for optimal retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_chunk_pdf(pdf_path):\n",
    "    loader = PyMuPDFLoader(pdf_path)\n",
    "    pages = loader.load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=50)\n",
    "    chunked_docs = []\n",
    "    for i, page in enumerate(pages):\n",
    "        page_number = i + 1\n",
    "        chapter = get_chapter_by_page(page_number)\n",
    "        chunks = splitter.split_documents([page])\n",
    "        for j, chunk in enumerate(chunks):\n",
    "            chunk.metadata = {\n",
    "                \"chapter\": chapter,\n",
    "                \"page_number\": page_number,\n",
    "                \"labels\": detect_labels(chunk.page_content),\n",
    "                \"book_name\": pdf_path.split(\"/\")[-1],\n",
    "                \"chunk_id\": f\"{chapter[:20].replace(' ', '_')}-{page_number}-{j}\"\n",
    "            }\n",
    "            chunked_docs.append(chunk)\n",
    "    return chunked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5477a844",
   "metadata": {},
   "source": [
    "## 🧬 Medical Embeddings Setup\n",
    "\n",
    "Initialize the medical-specific embedding model.\n",
    "Using S-PubMedBert-MS-MARCO which is fine-tuned specifically for medical text retrieval.\n",
    "This model understands medical terminology and relationships better than general-purpose embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131e8363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe30b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformerEmbeddings(\n",
    "    model_name=\"pritamdeka/S-PubMedBert-MS-MARCO\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44930c04",
   "metadata": {},
   "source": [
    "## 📚 Document Loading and Processing\n",
    "\n",
    "Load the medical textbook PDF and create document chunks.\n",
    "Update the pdf_path variable to point to your specific medical textbook file.\n",
    "The system will automatically process all pages and create searchable chunks with metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0683b99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example PDF path (adjust to your file)\n",
    "pdf_path = \"/kaggle/input/vishramsingh/Vishram Singh Textbook of Anatomy Vol 2.pdf\"\n",
    "\n",
    "# ✅ Load and chunk the PDF\n",
    "chunks = load_and_chunk_pdf(pdf_path)\n",
    "print(f\"[✅] Loaded and chunked: {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cdddd4",
   "metadata": {},
   "source": [
    "## 🗄️ Qdrant Vector Database Setup\n",
    "\n",
    "Initialize Qdrant client and create a collection for medical documents.\n",
    "Qdrant provides:\n",
    "- Efficient vector search with COSINE similarity\n",
    "- Rich metadata filtering capabilities\n",
    "- Scalable performance for large document collections\n",
    "- Persistent storage (when using host/port instead of :memory:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ca5685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Qdrant client and create collection\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "\n",
    "# Initialize Qdrant client (local instance)\n",
    "qdrant_client = QdrantClient(\":memory:\")  # Use in-memory for testing, or specify host/port for persistent\n",
    "\n",
    "# Create collection with appropriate vector size (768 for S-PubMedBert)\n",
    "collection_name = \"medical_documents\"\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=768, distance=Distance.COSINE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a4235c",
   "metadata": {},
   "source": [
    "## 📊 Vector Store Initialization\n",
    "\n",
    "Create the Qdrant vector store and populate it with medical document chunks.\n",
    "This step:\n",
    "- Generates embeddings for all document chunks\n",
    "- Stores vectors and metadata in Qdrant\n",
    "- Enables semantic search capabilities\n",
    "- Preserves all metadata for filtering and attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a98348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Use Qdrant vector database with medical documents\n",
    "vectorstore = Qdrant(\n",
    "    client=qdrant_client,\n",
    "    collection_name=collection_name,\n",
    "    embeddings=embedding_model\n",
    ")\n",
    "\n",
    "# Add documents to Qdrant\n",
    "vectorstore.add_documents(chunks)\n",
    "print(f\"[✅] Added {len(chunks)} chunks to Qdrant vector database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869f71bd",
   "metadata": {},
   "source": [
    "## 🧠 Query Decomposition Model\n",
    "\n",
    "Load FLAN-T5 model for intelligent query decomposition.\n",
    "This model breaks down complex medical questions into simpler, focused sub-questions\n",
    "that can be answered more accurately by the retrieval system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e4b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import re\n",
    "\n",
    "# Load the FLAN-T5 model for text2text-generation\n",
    "subq_pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cee74b",
   "metadata": {},
   "source": [
    "## 🔍 Advanced Query Decomposition Function\n",
    "\n",
    "Implement sophisticated query decomposition with:\n",
    "- Medical question parsing and pronoun resolution\n",
    "- Example-based prompting for consistent sub-question generation\n",
    "- Semantic deduplication to avoid redundant sub-questions\n",
    "- Keyword-based filtering to ensure diverse coverage\n",
    "\n",
    "This ensures complex medical questions are broken down into manageable, non-overlapping components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb10c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subquestionss(query):\n",
    "    \n",
    "    # Extract noun phrase for pronoun replacement\n",
    "    noun_match = re.search(r\"what\\s+is\\s+(.*?)(?:\\s*(,|and|\\.|\\?)|$)\", query, re.IGNORECASE)\n",
    "    subject = noun_match.group(1).strip() if noun_match else None\n",
    "\n",
    "    # Construct prompt\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful medical assistant.\n",
    "\n",
    "Break down each medical question into smaller subquestions that cover one clear medical concept at a time.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Question: Tell me about lung surfaces, borders and structures surrounding it.\n",
    "Subquestions:\n",
    "1. What are the surfaces of the lungs?\n",
    "2. What are the borders of the lungs?\n",
    "3. What are the structures surrounding the lungs?\n",
    "\n",
    "Question: Tell me about liver anatomy and function.\n",
    "Subquestions:\n",
    "1. What is the anatomy of the liver?\n",
    "2. What are the functions of the liver?\n",
    "\n",
    "Question: Tell me about the surfaces, borders and relations of the liver.\n",
    "Subquestions:\n",
    "1. What are the surfaces of the liver?\n",
    "2. What are the borders of the liver?\n",
    "3. What are the relations of the liver?\n",
    "\n",
    "Question: {query}\n",
    "Subquestions:\n",
    "\"\"\"\n",
    "\n",
    "    # Generate subquestions using BioFLAN\n",
    "    output = subq_pipe(prompt, max_new_tokens=200, do_sample=False)[0][\"generated_text\"]\n",
    "    raw = re.findall(r\"\\d+\\.\\s*([^0-9]+(?:\\?.*?)?)\", output)\n",
    "\n",
    "    # Basic cleanup and pronoun replacement\n",
    "    cleaned = []\n",
    "    seen_normalized = set()\n",
    "\n",
    "    for q in raw:\n",
    "        q_clean = q.strip()\n",
    "\n",
    "        if subject:\n",
    "            q_clean = re.sub(r\"\\bits\\b\", f\"the {subject}\", q_clean, flags=re.IGNORECASE)\n",
    "            q_clean = re.sub(r\"\\bit\\b\", f\"the {subject}\", q_clean, flags=re.IGNORECASE)\n",
    "            q_clean = re.sub(r\"\\btheir\\b\", f\"the {subject}'s\", q_clean, flags=re.IGNORECASE)\n",
    "\n",
    "        # Fix repeated \"the the\"\n",
    "        q_clean = re.sub(r\"\\bthe\\s+the\\b\", \"the\", q_clean, flags=re.IGNORECASE)\n",
    "\n",
    "        # Normalize for character-level deduplication\n",
    "        norm = re.sub(r\"[^a-z]\", \"\", q_clean.lower())\n",
    "        if norm not in seen_normalized:\n",
    "            cleaned.append(q_clean)\n",
    "            seen_normalized.add(norm)\n",
    "\n",
    "    # --- 🔍 Semantic Deduplication Using Keyword Sets ---\n",
    "    stopwords = {\"what\", \"are\", \"is\", \"the\", \"of\", \"in\", \"its\", \"a\", \"an\", \"and\", \"on\"}\n",
    "    def keyword_set(text):\n",
    "        tokens = re.findall(r\"\\w+\", text.lower())\n",
    "        return set(t for t in tokens if t not in stopwords)\n",
    "\n",
    "    final = []\n",
    "    seen_keywords = []\n",
    "\n",
    "    for q in cleaned:\n",
    "        q_keywords = keyword_set(q)\n",
    "\n",
    "        if any(q_keywords == existing for existing in seen_keywords):\n",
    "            continue  # Skip semantically duplicate question\n",
    "\n",
    "        final.append(q)\n",
    "        seen_keywords.append(q_keywords)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565dca6f",
   "metadata": {},
   "source": [
    "## 🎯 Reranking Model Setup\n",
    "\n",
    "Initialize BGE reranker for improved document relevance scoring.\n",
    "This cross-encoder model provides more accurate relevance scores\n",
    "by considering the interaction between query and document content,\n",
    "leading to better retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc922aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = CrossEncoder(\"BAAI/bge-reranker-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8756098",
   "metadata": {},
   "source": [
    "## 🔄 Retrieval and Reranking Pipeline\n",
    "\n",
    "Implement the core retrieval function that:\n",
    "1. Decomposes complex queries into sub-questions\n",
    "2. Retrieves relevant documents for each sub-question\n",
    "3. Reranks results using cross-encoder for better relevance\n",
    "4. Returns top documents organized by sub-question\n",
    "\n",
    "This two-stage approach ensures high-quality document retrieval for medical question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cef2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_rerank_by_subquery(query):\n",
    "\n",
    "  import time\n",
    "  start_time = time.time()\n",
    "  print(\"[🔍] Extracting subquestions...\")\n",
    "  subqueries = extract_subquestionss(query)\n",
    "  print(f\"[✅] Found {len(subqueries)} subquestions: {subqueries}\")\n",
    "\n",
    "  subquery_doc_map = {}\n",
    "  print(subqueries)\n",
    "\n",
    "  for sub in subqueries:\n",
    "    print(sub)\n",
    "    print(f\"[🔎] Retrieving for subquestion: '{sub}'\")\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 15})\n",
    "    docs = retriever.get_relevant_documents(\"query: \" + sub)\n",
    "    scores = reranker.predict([(sub, doc.page_content) for doc in docs])\n",
    "    top_docs = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)[:2]  # top 2\n",
    "    subquery_doc_map[sub] = [doc for doc, _ in top_docs]\n",
    "\n",
    "  print(\"[⏱️] Retrieval + reranking took {:.2f} seconds\".format(time.time() - start_time))\n",
    "  \n",
    "  return subquery_doc_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c980fe2d",
   "metadata": {},
   "source": [
    "## 🤖 Zephyr Language Model Setup\n",
    "\n",
    "Initialize the Zephyr-7B model for medical answer generation.\n",
    "Configuration:\n",
    "- Auto device mapping for multi-GPU setups\n",
    "- Float16 precision for memory efficiency\n",
    "- Evaluation mode for inference\n",
    "\n",
    "Zephyr is optimized for instruction following and produces high-quality, contextual responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27732d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",                # Spread across both GPUs\n",
    "    torch_dtype=torch.float16         # Full FP16 precision\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af157927",
   "metadata": {},
   "source": [
    "## ✍️ Medical Answer Generation Function\n",
    "\n",
    "Generate accurate medical answers for sub-questions using retrieved documents.\n",
    "Features:\n",
    "- Structured context formatting from relevant documents\n",
    "- Medical-specific prompting for accuracy\n",
    "- Controlled generation parameters (temperature=0.3 for consistency)\n",
    "- Source-aware responses that stick to provided context\n",
    "\n",
    "The function ensures answers are grounded in the retrieved medical textbook content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63fbfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_per_subquery(subquery, docs):\n",
    "    import time\n",
    "    t_start = time.time()\n",
    "\n",
    "    print(f\"[📥] Preparing context for subquery: '{subquery}'\")\n",
    "    \n",
    "    # Create structured bullet-point context from relevant documents\n",
    "    context = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        passage = doc.page_content.strip().replace(\"\\n\", \" \")\n",
    "        context.append(f\"- {passage}\")\n",
    "\n",
    "    # Chat-formatted prompt (optimized for Zephyr)\n",
    "    prompt = f\"\"\"<|system|>\n",
    "You are a helpful and precise medical assistant. Use only the provided medical textbook excerpts to answer the user's question. \n",
    "Never add outside knowledge. If information is missing, state that clearly.\n",
    "\n",
    "<|user|>\n",
    "Question:\n",
    "{subquery}\n",
    "\n",
    "Medical Textbook Sources:\n",
    "{chr(10).join(context)}\n",
    "\n",
    "Provide a clear, concise, and medically accurate answer.\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "    print(\"[🧠] Tokenizing prompt...\")\n",
    "    device = model.device\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(device)\n",
    "\n",
    "    print(\"[🚀] Generating answer with Zephyr...\")\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=400,\n",
    "            temperature=0.3,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    print(f\"[✅] Done with subquery: '{subquery}' in {time.time() - t_start:.2f} sec\")\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True).split(\"<|assistant|>\")[-1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f45c87",
   "metadata": {},
   "source": [
    "## 🎯 Complete Medical Query Processing Pipeline\n",
    "\n",
    "The main function that orchestrates the entire medical RAG pipeline:\n",
    "1. **Query Analysis**: Process the input medical question\n",
    "2. **Decomposition**: Break complex questions into focused sub-questions\n",
    "3. **Retrieval**: Find relevant documents for each sub-question\n",
    "4. **Reranking**: Score and rank documents by relevance\n",
    "5. **Generation**: Generate comprehensive answers using Zephyr\n",
    "6. **Synthesis**: Combine results into a coherent response\n",
    "\n",
    "This end-to-end pipeline ensures accurate, well-sourced medical answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6c264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_medical_query(query):\n",
    "    results = {}\n",
    "\n",
    "    print(f\"\\n[🧠] Starting query: '{query}'\")\n",
    "\n",
    "    # ✅ Use your dedicated retrieval + reranking function\n",
    "    subquery_docs = retrieve_and_rerank_by_subquery(query)\n",
    "\n",
    "    # ✅ Loop through each subquery and generate answers\n",
    "    for i, (subq, docs) in enumerate(subquery_docs.items(), start=1):\n",
    "        print(f\"\\n🧩 Subquestion {i}/{len(subquery_docs)}: {subq}\")\n",
    "\n",
    "        # Generate answer for each subquery separately\n",
    "        answer = generate_answer_per_subquery(subq, docs)\n",
    "\n",
    "        # Print and store the answer\n",
    "        print(f\"\\n📘 Answer:\\n{answer}\\n\")\n",
    "        results[subq] = answer\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example\n",
    "query = \"What is the inguinal canal, and what are its contents?\"\n",
    "answers = answer_medical_query(query)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
